{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfed42a6",
   "metadata": {},
   "source": [
    "\n",
    "# Pipeline de Coleta → Extração → Limpeza → Deduplicação → Shards (Notebook)\n",
    "\n",
    "Este notebook executa o pipeline em **etapas**, permitindo inspeção e depuração fase a fase.\n",
    "\n",
    "> **Tema**: violência doméstica no contexto da atuação policial  \n",
    "> **Entrada**: `crawl_manifest.csv` (manifesto deduplicado com URLs)  \n",
    "> **Saída**: diretório `OUTDIR/` com brutos (`data/raw`), textos limpos (`data/text`), shards JSONL (`data/shards`) e relatórios (`logs/`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad69763",
   "metadata": {},
   "source": [
    "## 1) Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26790dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Caminhos\n",
    "MANIFEST = Path(\"crawl_manifest.csv\")  # ajuste se necessário\n",
    "OUTDIR = Path(\"corpus_out_nb\")\n",
    "\n",
    "# Parâmetros\n",
    "RATE = 1.0                # requisições por segundo (global)\n",
    "MAX_WORKERS = 2           # threads de download/extração\n",
    "TIMEOUT = 15              # timeout por requisição (s)\n",
    "MIN_CHARS = 300           # mínimo de caracteres por doc (após limpeza)\n",
    "SHARD_SIZE_MB = 100.0     # tamanho alvo de cada shard\n",
    "LOG_LEVEL = \"INFO\"        # \"DEBUG\" para mais verbosidade\n",
    "USER_AGENT = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119 Safari/537.36\"\n",
    "\n",
    "# Preparar diretórios de saída\n",
    "for sub in [\"data/raw\", \"data/text\", \"data/shards\", \"logs\"]:\n",
    "    (OUTDIR / sub).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"OUTDIR:\", OUTDIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a991f",
   "metadata": {},
   "source": [
    "## 2) Imports e utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv, re, json, time, hashlib, logging\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Dependências opcionais (tratadas com fallback)\n",
    "try:\n",
    "    import trafilatura\n",
    "except Exception:\n",
    "    trafilatura = None\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except Exception:\n",
    "    fitz = None\n",
    "\n",
    "try:\n",
    "    from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "except Exception:\n",
    "    pdfminer_extract_text = None\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=getattr(logging, LOG_LEVEL.upper(), logging.INFO),\n",
    "                    format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "log = logging.getLogger(\"pipeline_nb\")\n",
    "\n",
    "# Sessão HTTP com retries e headers de navegador\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "def build_session(timeout: int, user_agent: str):\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.8, status_forcelist=[429,500,502,503,504])\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": user_agent,\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"pt-BR,pt;q=0.9,en;q=0.8\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    })\n",
    "    orig = s.request\n",
    "    def wrapped(method, url, **kwargs):\n",
    "        kwargs.setdefault(\"timeout\", timeout)\n",
    "        return orig(method, url, **kwargs)\n",
    "    s.request = wrapped\n",
    "    return s\n",
    "\n",
    "session = build_session(TIMEOUT, USER_AGENT)\n",
    "\n",
    "def sha1(s: str) -> str: return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "def sha256(s: str) -> str: return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def norm_url(u: str) -> str:\n",
    "    p = urlparse(u); path = p.path.rstrip(\"/\")\n",
    "    return urlunparse((p.scheme, p.netloc.lower(), path, \"\", \"\", \"\"))\n",
    "\n",
    "# Rate limiter simples (token bucket)\n",
    "class RateLimiter:\n",
    "    def __init__(self, rate_per_sec: float):\n",
    "        import threading\n",
    "        self.rate = max(rate_per_sec, 0.1)\n",
    "        self.tokens = self.rate\n",
    "        self.last = time.monotonic()\n",
    "        self.lock = threading.Lock()\n",
    "    def acquire(self):\n",
    "        with self.lock:\n",
    "            while True:\n",
    "                now = time.monotonic()\n",
    "                elapsed = now - self.last\n",
    "                self.tokens = min(self.rate, self.tokens + elapsed * self.rate)\n",
    "                self.last = now\n",
    "                if self.tokens >= 1.0:\n",
    "                    self.tokens -= 1.0\n",
    "                    return\n",
    "                time.sleep((1.0 - self.tokens) / self.rate)\n",
    "\n",
    "limiter = RateLimiter(RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dfa223",
   "metadata": {},
   "source": [
    "## 3) Checagem de `robots.txt` (com timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib import robotparser\n",
    "\n",
    "class RobotsCache:\n",
    "    \"\"\"Busca robots.txt com requests e timeout; se falhar, assume permitido.\"\"\"\n",
    "    def __init__(self, user_agent: str, timeout: int = 15):\n",
    "        self.user_agent = user_agent\n",
    "        self.timeout = timeout\n",
    "        self.cache = {}\n",
    "    def can_fetch(self, url: str) -> bool:\n",
    "        p = urlparse(url)\n",
    "        base = f\"{p.scheme}://{p.netloc}\"\n",
    "        if base in self.cache:\n",
    "            rp = self.cache[base]\n",
    "            if rp is None: return True\n",
    "            try:\n",
    "                return rp.can_fetch(self.user_agent, url)\n",
    "            except Exception:\n",
    "                return True\n",
    "        robots_url = base + \"/robots.txt\"\n",
    "        rp = robotparser.RobotFileParser()\n",
    "        try:\n",
    "            resp = session.get(robots_url)  # session já tem timeout\n",
    "            if resp.status_code != 200 or not resp.content:\n",
    "                self.cache[base] = None\n",
    "                return True\n",
    "            rp.parse(resp.text.splitlines())\n",
    "            self.cache[base] = rp\n",
    "            return rp.can_fetch(self.user_agent, url)\n",
    "        except Exception:\n",
    "            self.cache[base] = None\n",
    "            return True\n",
    "\n",
    "robots = RobotsCache(USER_AGENT, timeout=TIMEOUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8beb51c",
   "metadata": {},
   "source": [
    "## 4) Carregar manifesto e deduplicar URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13258835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(MANIFEST)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "df[\"url_norm\"] = df[\"url\"].apply(norm_url)\n",
    "\n",
    "dedup = df.drop_duplicates(subset=[\"url_norm\"]).reset_index(drop=True)\n",
    "len(df), len(dedup), dedup.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62158f",
   "metadata": {},
   "source": [
    "## 5) Funções de extração e limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b\", re.IGNORECASE)\n",
    "CPF_RE = re.compile(r\"\\b\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}\\b\")\n",
    "CNPJ_RE = re.compile(r\"\\b\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2}\\b\")\n",
    "MULTISPACE = re.compile(r\"[ \\t]+\")\n",
    "MULTINEWLINE = re.compile(r\"\\n{3,}\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = s.replace(\"\\ufeff\", \"\").replace(\"-\\n\",\"\")\n",
    "    s = MULTISPACE.sub(\" \", s)\n",
    "    s = MULTINEWLINE.sub(\"\\n\\n\", s)\n",
    "    s = EMAIL_RE.sub(\"<EMAIL>\", s)\n",
    "    s = CPF_RE.sub(\"<CPF>\", s)\n",
    "    s = CNPJ_RE.sub(\"<CNPJ>\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_html_main(html_bytes: bytes) -> str:\n",
    "    if trafilatura is not None:\n",
    "        try:\n",
    "            txt = trafilatura.extract(html_bytes, include_comments=False, include_tables=False, favor_recall=True)\n",
    "            if txt and txt.strip():\n",
    "                return txt.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_bytes, \"lxml\")\n",
    "        for tag in soup([\"script\",\"style\",\"nav\",\"header\",\"footer\",\"noscript\",\"aside\"]):\n",
    "            tag.decompose()\n",
    "        return soup.get_text(\"\\n\").strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    if fitz is not None:\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            parts = [page.get_text(\"text\") for page in doc]\n",
    "            out = \"\\n\".join(parts)\n",
    "            if out.strip():\n",
    "                return out.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if pdfminer_extract_text is not None:\n",
    "        try:\n",
    "            out = pdfminer_extract_text(pdf_path) or \"\"\n",
    "            return out.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6876d",
   "metadata": {},
   "source": [
    "## 6) Baixar e extrair (HTML/PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DIR = OUTDIR / \"data\" / \"raw\"\n",
    "TEXT_DIR = OUTDIR / \"data\" / \"text\"\n",
    "\n",
    "results = []\n",
    "\n",
    "def fetch_and_extract(row):\n",
    "    url = row[\"url\"]\n",
    "    # O true pula a verificação de robots.txt\n",
    "    if not true:#robots.can_fetch(url):\n",
    "        return {\"url\": url, \"status\": \"robots_disallow\"}\n",
    "    limiter.acquire()\n",
    "    try:\n",
    "        resp = session.get(url)\n",
    "        ct = resp.headers.get(\"Content-Type\", \"\")\n",
    "        content = resp.content\n",
    "        status = resp.status_code\n",
    "    except Exception as e:\n",
    "        return {\"url\": url, \"status\": f\"fetch_error:{e}\"}\n",
    "    if status != 200 or not content:\n",
    "        return {\"url\": url, \"status\": f\"http_{status}\"}\n",
    "    is_pdf = (\"pdf\" in (ct or \"\").lower()) or url.lower().endswith(\".pdf\")\n",
    "    ext = \".pdf\" if is_pdf else \".html\"\n",
    "    fname = sha256(row[\"url_norm\"])[:20] + ext\n",
    "    raw_path = RAW_DIR / fname\n",
    "    try:\n",
    "        raw_path.write_bytes(content)\n",
    "    except Exception as e:\n",
    "        return {\"url\": url, \"status\": f\"save_raw_error:{e}\"}\n",
    "    try:\n",
    "        text = extract_pdf_text(str(raw_path)) if is_pdf else extract_html_main(content)\n",
    "    except Exception as e:\n",
    "        text = \"\"\n",
    "    text = clean_text(text)\n",
    "    if len(text) < MIN_CHARS:\n",
    "        return {\"url\": url, \"status\": \"too_short\", \"chars\": len(text)}\n",
    "    tname = fname.replace(ext, \".txt\")\n",
    "    text_path = TEXT_DIR / tname\n",
    "    try:\n",
    "        text_path.write_text(text, encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        return {\"url\": url, \"status\": f\"save_text_error:{e}\"}\n",
    "    return {\"url\": url, \"status\": \"ok\", \"text_path\": str(text_path), \"chars\": len(text)}\n",
    "\n",
    "# Execução (paralela)\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futs = [ex.submit(fetch_and_extract, r) for r in dedup.to_dict(orient=\"records\")]\n",
    "    for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Coletando/Extraindo\"):\n",
    "        results.append(fut.result())\n",
    "\n",
    "len(results), results[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e34d11",
   "metadata": {},
   "source": [
    "## 7) Deduplicar por hash e gerar shards JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072994d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "SHARDS_DIR = OUTDIR / \"data\" / \"shards\"\n",
    "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "seen_hash = set()\n",
    "shard_bytes_limit = int(SHARD_SIZE_MB * 1024 * 1024)\n",
    "buf = []; size = 0; shard_id = 0\n",
    "\n",
    "def flush():\n",
    "    global buf, size, shard_id\n",
    "    if not buf: return\n",
    "    out_path = SHARDS_DIR / f\"corpus_{shard_id:04d}.jsonl\"\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for line in buf:\n",
    "            f.write(line + \"\\n\")\n",
    "    buf = []; size = 0; shard_id += 1\n",
    "    print(\"Shard salvo:\", out_path)\n",
    "\n",
    "ok_items = [r for r in results if r.get(\"status\") == \"ok\"]\n",
    "for item in ok_items:\n",
    "    try:\n",
    "        text = Path(item[\"text_path\"]).read_text(encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        continue\n",
    "    h = sha1(text)\n",
    "    if h in seen_hash:\n",
    "        continue\n",
    "    seen_hash.add(h)\n",
    "    line = json.dumps({\"text\": text}, ensure_ascii=False)\n",
    "    buf.append(line); size += len(line.encode(\"utf-8\"))\n",
    "    if size >= shard_bytes_limit:\n",
    "        flush()\n",
    "flush()\n",
    "\n",
    "len(ok_items), len(seen_hash)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2652e5",
   "metadata": {},
   "source": [
    "## 8) Relatório final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "REPORT = OUTDIR / \"logs\" / \"report.json\"\n",
    "stats = {\n",
    "    \"total_urls\": int(len(dedup)),\n",
    "    \"ok\": int(sum(1 for r in results if r.get(\"status\") == \"ok\")),\n",
    "    \"too_short\": int(sum(1 for r in results if r.get(\"status\") == \"too_short\")),\n",
    "    \"http_non200\": int(sum(1 for r in results if str(r.get(\"status\",\"\")).startswith(\"http_\"))),\n",
    "    \"errors\": [r for r in results if \"error\" in r.get(\"status\",\"\")],\n",
    "    \"shards\": int(len(list((OUTDIR / \"data\" / \"shards\").glob(\"corpus_*.jsonl\")))),\n",
    "}\n",
    "REPORT.write_text(json.dumps(stats, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0900293",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Dicas\n",
    "- Se um host estiver lento, ajuste: `MAX_WORKERS = 1`, `RATE = 0.5`, `TIMEOUT = 10`.\n",
    "- Para debug detalhado: `LOG_LEVEL = \"DEBUG\"`.\n",
    "- Se precisar **ignorar robots** para diagnóstico rápido, troque `robots.can_fetch(url)` para sempre `True` (apenas para teste).\n",
    "- Para enriquecer metadados, crie um `index.csv` registrando `url`, `hash`, `path` e `chars` durante o loop.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
